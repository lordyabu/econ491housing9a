{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c850b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of https://arxiv.org/abs/2101.09460\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from collections import deque\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eedc1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_raw = pd.read_csv('./data/train_with_dummies.csv', index_col=[0])\n",
    "\n",
    "# Specify prefixes of columns to drop\n",
    "prefixes_to_drop = ['Id', 'SaleType', 'SaleCondition', 'SalePrice']\n",
    "\n",
    "# Drop specified columns before imputation\n",
    "df_filtered = df_raw.drop([col for col in df_raw.columns if any(col.startswith(prefix) for prefix in prefixes_to_drop)], axis=1)\n",
    "\n",
    "# Impute missing values in the filtered dataset\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df_filtered), columns=df_filtered.columns)\n",
    "\n",
    "# Extract the SalePrice column from the original dataset for use as the target variable\n",
    "sale_price_col = df_raw['SalePrice']\n",
    "sale_price_mean = np.mean(sale_price_col)\n",
    "\n",
    "# Scale the imputed dataset\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_imputed)\n",
    "df_scaled = pd.DataFrame(scaled_data, columns=df_imputed.columns)\n",
    "\n",
    "# Define data_x and data_y for model input\n",
    "data_x = df_scaled\n",
    "data_y = sale_price_col.reset_index(drop=True)  # Reset index to ensure alignment\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "column_to_index = {column: index for index, column in enumerate(data_x.columns)}\n",
    "index_to_column = {index: column for column, index in column_to_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14ead15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolQC_Fa</th>\n",
       "      <th>PoolQC_Gd</th>\n",
       "      <th>Fence_GdPrv</th>\n",
       "      <th>Fence_GdWo</th>\n",
       "      <th>Fence_MnPrv</th>\n",
       "      <th>Fence_MnWw</th>\n",
       "      <th>MiscFeature_Gar2</th>\n",
       "      <th>MiscFeature_Othr</th>\n",
       "      <th>MiscFeature_Shed</th>\n",
       "      <th>MiscFeature_TenC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.073375</td>\n",
       "      <td>-0.229372</td>\n",
       "      <td>-0.207142</td>\n",
       "      <td>0.651479</td>\n",
       "      <td>-0.517200</td>\n",
       "      <td>1.050994</td>\n",
       "      <td>0.878668</td>\n",
       "      <td>0.511418</td>\n",
       "      <td>0.575425</td>\n",
       "      <td>-0.288653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>-0.205214</td>\n",
       "      <td>-0.195977</td>\n",
       "      <td>-0.347118</td>\n",
       "      <td>-0.087129</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.186352</td>\n",
       "      <td>-0.02618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.872563</td>\n",
       "      <td>0.451936</td>\n",
       "      <td>-0.091886</td>\n",
       "      <td>-0.071836</td>\n",
       "      <td>2.179628</td>\n",
       "      <td>0.156734</td>\n",
       "      <td>-0.429577</td>\n",
       "      <td>-0.574410</td>\n",
       "      <td>1.171992</td>\n",
       "      <td>-0.288653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>-0.205214</td>\n",
       "      <td>-0.195977</td>\n",
       "      <td>-0.347118</td>\n",
       "      <td>-0.087129</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.186352</td>\n",
       "      <td>-0.02618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.073375</td>\n",
       "      <td>-0.093110</td>\n",
       "      <td>0.073480</td>\n",
       "      <td>0.651479</td>\n",
       "      <td>-0.517200</td>\n",
       "      <td>0.984752</td>\n",
       "      <td>0.830215</td>\n",
       "      <td>0.323060</td>\n",
       "      <td>0.092907</td>\n",
       "      <td>-0.288653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>-0.205214</td>\n",
       "      <td>-0.195977</td>\n",
       "      <td>-0.347118</td>\n",
       "      <td>-0.087129</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.186352</td>\n",
       "      <td>-0.02618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.309859</td>\n",
       "      <td>-0.456474</td>\n",
       "      <td>-0.096897</td>\n",
       "      <td>0.651479</td>\n",
       "      <td>-0.517200</td>\n",
       "      <td>-1.863632</td>\n",
       "      <td>-0.720298</td>\n",
       "      <td>-0.574410</td>\n",
       "      <td>-0.499274</td>\n",
       "      <td>-0.288653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>-0.205214</td>\n",
       "      <td>-0.195977</td>\n",
       "      <td>-0.347118</td>\n",
       "      <td>-0.087129</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.186352</td>\n",
       "      <td>-0.02618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.073375</td>\n",
       "      <td>0.633618</td>\n",
       "      <td>0.375148</td>\n",
       "      <td>1.374795</td>\n",
       "      <td>-0.517200</td>\n",
       "      <td>0.951632</td>\n",
       "      <td>0.733308</td>\n",
       "      <td>1.364570</td>\n",
       "      <td>0.463568</td>\n",
       "      <td>-0.288653</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.045376</td>\n",
       "      <td>-0.205214</td>\n",
       "      <td>-0.195977</td>\n",
       "      <td>-0.347118</td>\n",
       "      <td>-0.087129</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.037037</td>\n",
       "      <td>-0.186352</td>\n",
       "      <td>-0.02618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 273 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
       "0    0.073375    -0.229372 -0.207142     0.651479    -0.517200   1.050994   \n",
       "1   -0.872563     0.451936 -0.091886    -0.071836     2.179628   0.156734   \n",
       "2    0.073375    -0.093110  0.073480     0.651479    -0.517200   0.984752   \n",
       "3    0.309859    -0.456474 -0.096897     0.651479    -0.517200  -1.863632   \n",
       "4    0.073375     0.633618  0.375148     1.374795    -0.517200   0.951632   \n",
       "\n",
       "   YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  PoolQC_Fa  \\\n",
       "0      0.878668    0.511418    0.575425   -0.288653  ...  -0.037037   \n",
       "1     -0.429577   -0.574410    1.171992   -0.288653  ...  -0.037037   \n",
       "2      0.830215    0.323060    0.092907   -0.288653  ...  -0.037037   \n",
       "3     -0.720298   -0.574410   -0.499274   -0.288653  ...  -0.037037   \n",
       "4      0.733308    1.364570    0.463568   -0.288653  ...  -0.037037   \n",
       "\n",
       "   PoolQC_Gd  Fence_GdPrv  Fence_GdWo  Fence_MnPrv  Fence_MnWw  \\\n",
       "0  -0.045376    -0.205214   -0.195977    -0.347118   -0.087129   \n",
       "1  -0.045376    -0.205214   -0.195977    -0.347118   -0.087129   \n",
       "2  -0.045376    -0.205214   -0.195977    -0.347118   -0.087129   \n",
       "3  -0.045376    -0.205214   -0.195977    -0.347118   -0.087129   \n",
       "4  -0.045376    -0.205214   -0.195977    -0.347118   -0.087129   \n",
       "\n",
       "   MiscFeature_Gar2  MiscFeature_Othr  MiscFeature_Shed  MiscFeature_TenC  \n",
       "0         -0.037037         -0.037037         -0.186352          -0.02618  \n",
       "1         -0.037037         -0.037037         -0.186352          -0.02618  \n",
       "2         -0.037037         -0.037037         -0.186352          -0.02618  \n",
       "3         -0.037037         -0.037037         -0.186352          -0.02618  \n",
       "4         -0.037037         -0.037037         -0.186352          -0.02618  \n",
       "\n",
       "[5 rows x 273 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bad010f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim, output_dim):\n",
    "    model = models.Sequential([\n",
    "        layers.Dense(128, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(output_dim)\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')\n",
    "    \n",
    "    target_model = models.clone_model(model)\n",
    "    target_model.set_weights(model.get_weights())\n",
    "    \n",
    "    return model, target_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, batch_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95    # discount rate\n",
    "        self.epsilon = 1.0  # exploration rate\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.batch_size = batch_size\n",
    "        self.model, self.target_model = create_model(state_size, action_size)\n",
    "\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # Perform a random action, but only consider valid actions\n",
    "            valid_actions = [i for i, val in enumerate(state[0]) if val == 0]\n",
    "            return np.random.choice(valid_actions)\n",
    "        else:\n",
    "            act_values = self.model.predict(state, verbose=0)\n",
    "            # Mask out invalid actions by setting their Q-values to a large negative number\n",
    "            act_values[0][state[0] == 1] = -1e9\n",
    "            return np.argmax(act_values[0])  # returns action\n",
    "\n",
    "        \n",
    "    def replay(self):\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = []\n",
    "        next_states = []\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            # Reshape state and next_state without the extra singleton dimension\n",
    "            state = np.squeeze(state)  # Remove the extra singleton dimension\n",
    "            next_state = np.squeeze(next_state)  # Remove the extra singleton dimension\n",
    "            states.append(state)\n",
    "            next_states.append(next_state)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        current_q_values = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = (reward + self.gamma * np.amax(next_q_values[i]))\n",
    "            current_q_values[i][action] = target\n",
    "\n",
    "        # Set verbose=0 to hide progress bars\n",
    "        self.model.fit(states, current_q_values, epochs=1, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b58060ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelectionEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, num_features, data_x, data_y, error_metric='rmse', debug=False, max_variables=None, ranker=True):\n",
    "        super(FeatureSelectionEnv, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.data_x = data_x\n",
    "        self.data_y = data_y\n",
    "        self.sale_price_mean = 180921.19\n",
    "        self.action_space = spaces.Discrete(num_features + 1)  # Action: select a feature to toggle or END\n",
    "        self.observation_space = spaces.MultiBinary(num_features + 1)  # State: binary vector of selected features\n",
    "        self.state = None\n",
    "        self.debug = debug\n",
    "        self.error_metric = error_metric\n",
    "        self.max_variables = max_variables\n",
    "        \n",
    "        if self.error_metric == 'rmse':\n",
    "            self.scorer = make_scorer(mean_squared_error, squared=False, greater_is_better=False)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported error metric\")\n",
    "        self.num_cvs = 5\n",
    "        self.previous_error = self.get_initial_error()\n",
    "\n",
    "    def get_initial_error(self):\n",
    "        predictions = np.full(self.data_y.shape, self.sale_price_mean)\n",
    "        if self.error_metric == 'rmse':\n",
    "            initial_error = mean_squared_error(self.data_y, predictions, squared=False)\n",
    "        else:\n",
    "            initial_error = 0\n",
    "        print(initial_error)\n",
    "        return initial_error\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.zeros(self.num_features + 1, dtype=np.int8)  # Start with no features selected\n",
    "        self.previous_error = self.get_initial_error()\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        # Check if the maximum number of variables has been reached\n",
    "        if self.max_variables is not None and np.sum(self.state[:-1]) >= self.max_variables:\n",
    "            done = True\n",
    "            \n",
    "            # Change to evalue_reward if not that good\n",
    "            reward = self.evaluate_reward()  # Evaluate reward for reaching max variables\n",
    "        else:\n",
    "            all_features_selected = all(self.state[:-1])\n",
    "            if action == self.num_features or all_features_selected:\n",
    "                # Action is to end the sequence or all features selected\n",
    "                done = True\n",
    "                reward = self.evaluate_reward_with_bonus(action)\n",
    "            else:\n",
    "                if self.state[action] == 0:  # Ensure action is valid (feature not already included)\n",
    "                    self.state[action] = 1\n",
    "                    reward = self.evaluate_reward()  # Optionally, evaluate reward after each feature selection\n",
    "                else:\n",
    "                    raise ValueError(\"Shouldn't get to this because agent handles invalid action\")\n",
    "        \n",
    "#         if reward == -1000:\n",
    "#             done = True\n",
    "        info = {}\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def evaluate_reward_with_bonus(self, action):\n",
    "        # Convert action from column name to column index using column_to_index mapping\n",
    "        action_index = column_to_index[action]\n",
    "\n",
    "        selected_features_indices = [i for i, included in enumerate(self.state[:-1]) if included == 1]\n",
    "        X_selected = self.data_x.iloc[:, selected_features_indices]\n",
    "        model = LinearRegression()\n",
    "        scores = cross_val_score(model, X_selected, self.data_y, scoring=self.scorer, cv=self.num_cvs)\n",
    "        current_error = -np.mean(scores)\n",
    "        reward = self.previous_error - current_error\n",
    "        self.previous_error = current_error\n",
    "\n",
    "        # Define the bonus condition based on the action's column name\n",
    "        bonus_column_name = 'YearRemodAdd'  # Replace with the actual column name for bonus\n",
    "        bonus_reward = 10000  # Define the bonus reward amount\n",
    "\n",
    "        # Apply the bonus reward if the action is on the bonus column\n",
    "        if action == bonus_column_name:\n",
    "            reward += bonus_reward\n",
    "\n",
    "        if self.debug:\n",
    "            print(f\"Action: {action}, Reward: {reward}\")\n",
    "        return reward\n",
    "\n",
    "\n",
    "    def evaluate_reward(self):\n",
    "        selected_features_indices = [i for i, included in enumerate(self.state[:-1]) if included == 1]\n",
    "        X_selected = self.data_x.iloc[:, selected_features_indices]\n",
    "        model = LinearRegression()\n",
    "        scores = cross_val_score(model, X_selected, self.data_y, scoring=self.scorer, cv=self.num_cvs)\n",
    "        current_error = -np.mean(scores)\n",
    "        reward = self.previous_error - current_error\n",
    "        self.previous_error = current_error\n",
    "        \n",
    "        \n",
    "#         if current_error > 500000:\n",
    "#             reward = -1000\n",
    "        if self.debug:\n",
    "            print(reward)\n",
    "        return reward\n",
    "    \n",
    "    def evaluate_final_reward(self):\n",
    "        selected_features_indices = [i for i, included in enumerate(self.state[:-1]) if included == 1]\n",
    "        X_selected = self.data_x.iloc[:, selected_features_indices]\n",
    "        model = LinearRegression()\n",
    "        scores = cross_val_score(model, X_selected, self.data_y, scoring=self.scorer, cv=self.num_cvs)\n",
    "        current_error = -np.mean(scores)\n",
    "        reward = self.get_initial_error() - current_error\n",
    "        self.previous_error = current_error\n",
    "        if current_error > 500000:\n",
    "            reward = -1000\n",
    "        if self.debug:\n",
    "            print(reward)\n",
    "        return reward * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32d82e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79415.29188606772\n"
     ]
    }
   ],
   "source": [
    "env = FeatureSelectionEnv(num_features=273, data_x=data_x, data_y=data_y, error_metric='rmse', max_variables=20)\n",
    "agent = DQNAgent(env.observation_space.shape[0], env.action_space.n, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da9554c0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79415.29188606772\n",
      "Episode: 1, Total reward: 16928.71918584269, Exploration Rate: 1.0\n",
      "79415.29188606772\n",
      "Episode: 2, Total reward: 12231.65754566461, Exploration Rate: 1.0\n",
      "79415.29188606772\n",
      "Episode: 3, Total reward: 14484.525181259458, Exploration Rate: 1.0\n",
      "79415.29188606772\n",
      "Episode: 4, Total reward: 24456.091001331362, Exploration Rate: 1.0\n",
      "79415.29188606772\n",
      "Episode: 5, Total reward: 19113.13113576077, Exploration Rate: 1.0\n",
      "79415.29188606772\n",
      "Episode: 6, Total reward: 20500.794302414477, Exploration Rate: 1.0\n",
      "79415.29188606772\n",
      "Episode: 7, Total reward: 20205.540788083257, Exploration Rate: 1.0\n",
      "79415.29188606772\n",
      "Episode: 8, Total reward: 13352.099635115723, Exploration Rate: 1.0\n",
      "79415.29188606772\n",
      "Episode: 9, Total reward: 30154.836973367303, Exploration Rate: 1.0\n",
      "79415.29188606772\n",
      "Episode: 10, Total reward: 22274.572936136974, Exploration Rate: 1.0\n",
      "79415.29188606772\n",
      "Episode: 11, Total reward: 7061.981083564577, Exploration Rate: 1.0\n",
      "79415.29188606772\n",
      "Episode: 12, Total reward: 25973.365723862837, Exploration Rate: 1.0\n",
      "79415.29188606772\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "273",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m500\u001b[39m):  \u001b[38;5;66;03m# Assuming a max timestep per episode\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m---> 12\u001b[0m     next_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward  \u001b[38;5;66;03m# Accumulate reward\u001b[39;00m\n\u001b[0;32m     14\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(next_state, [\u001b[38;5;241m1\u001b[39m, env\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[21], line 53\u001b[0m, in \u001b[0;36mFeatureSelectionEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features \u001b[38;5;129;01mor\u001b[39;00m all_features_selected:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m# Action is to end the sequence or all features selected\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_reward_with_bonus\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[action] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# Ensure action is valid (feature not already included)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[21], line 72\u001b[0m, in \u001b[0;36mFeatureSelectionEnv.evaluate_reward_with_bonus\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_reward_with_bonus\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# Convert action from column name to column index using column_to_index mapping\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     action_index \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     74\u001b[0m     selected_features_indices \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i, included \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m included \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     75\u001b[0m     X_selected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_x\u001b[38;5;241m.\u001b[39miloc[:, selected_features_indices]\n",
      "\u001b[1;31mKeyError\u001b[0m: 273"
     ]
    }
   ],
   "source": [
    "NUM_EPISODES = 1000  # Number of episodes to run\n",
    "batch_size = 32\n",
    "total_rewards_list = []  # List to store total rewards for each episode\n",
    "\n",
    "for e in range(NUM_EPISODES):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "    total_reward = 0  # Initialize total reward for the episode\n",
    "\n",
    "    for time in range(500):  # Assuming a max timestep per episode\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward  # Accumulate reward\n",
    "        next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(f\"Episode: {e+1}, Total reward: {total_reward}, Exploration Rate: {agent.epsilon}\")  # Print total reward\n",
    "            total_rewards_list.append(total_reward)  # Append total reward to the list\n",
    "            break\n",
    "    \n",
    "    if e > 10:\n",
    "        agent.replay()\n",
    "\n",
    "# Plotting total rewards\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(total_rewards_list)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter out rewards that are 0 or less\n",
    "filtered_rewards = [reward for reward in total_rewards_list if reward]\n",
    "\n",
    "plt.plot(filtered_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward per Episode (Positive Rewards Only)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b540ace3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79415.29188606772\n",
      "Episode: 1, Total reward: -42185.32224540572\n"
     ]
    }
   ],
   "source": [
    "for e in range(1):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, env.observation_space.shape[0]])\n",
    "    total_reward = 0  # Initialize total reward for the episode\n",
    "\n",
    "    for time in range(500):  # Assuming a max timestep per episode\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward  # Accumulate reward\n",
    "        next_state = np.reshape(next_state, [1, env.observation_space.shape[0]])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(f\"Episode: {e+1}, Total reward: {total_reward}\")  # Print total reward\n",
    "            total_rewards_list.append(total_reward)  # Append total reward to the list\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4168af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected column: MSSubClass\n",
      "Selected column: LotFrontage\n",
      "Selected column: LotArea\n",
      "Selected column: OverallQual\n",
      "Selected column: OverallCond\n",
      "Selected column: YearBuilt\n",
      "Selected column: YearRemodAdd\n",
      "Selected column: MasVnrArea\n",
      "Selected column: BsmtFinSF1\n",
      "Selected column: BsmtFinSF2\n",
      "Selected column: BsmtUnfSF\n",
      "Selected column: TotalBsmtSF\n",
      "Selected column: 1stFlrSF\n",
      "Selected column: 2ndFlrSF\n",
      "Selected column: LowQualFinSF\n",
      "Selected column: GrLivArea\n",
      "Selected column: BsmtFullBath\n",
      "Selected column: BsmtHalfBath\n",
      "Selected column: FullBath\n",
      "Selected column: BsmtFinType2_Unf\n"
     ]
    }
   ],
   "source": [
    "for i, included in enumerate(env.state):\n",
    "    if included == 1:\n",
    "        print(\"Selected column:\", data_x.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bad324d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "\n",
    "\n",
    "# Selected column: MSSubClass\n",
    "# Selected column: LotArea\n",
    "# Selected column: OverallQual\n",
    "# Selected column: OverallCond\n",
    "# Selected column: YearBuilt\n",
    "# Selected column: TotalBsmtSF\n",
    "# Selected column: GrLivArea\n",
    "# Selected column: BedroomAbvGr\n",
    "# Selected column: TotRmsAbvGrd\n",
    "# Selected column: Fireplaces\n",
    "# Selected column: GarageCars\n",
    "# Selected column: ScreenPorch\n",
    "# Selected column: Alley_Grvl\n",
    "# Selected column: LandContour_Low\n",
    "# Selected column: LotConfig_CulDSac\n",
    "# Selected column: LotConfig_FR2\n",
    "# Selected column: Neighborhood_BrkSide\n",
    "# Selected column: Neighborhood_Crawfor\n",
    "# Selected column: Neighborhood_Mitchel\n",
    "# Selected column: Neighborhood_NAmes\n",
    "# Selected column: Neighborhood_NoRidge\n",
    "# Selected column: Neighborhood_NridgHt\n",
    "# Selected column: Neighborhood_Somerst\n",
    "# Selected column: Neighborhood_StoneBr\n",
    "# Selected column: Condition1_Norm\n",
    "# Selected column: BldgType_1Fam\n",
    "# Selected column: HouseStyle_1Story\n",
    "# Selected column: RoofStyle_Mansard\n",
    "# Selected column: RoofMatl_ClyTile\n",
    "# Selected column: RoofMatl_WdShngl\n",
    "# Selected column: Exterior1st_BrkComm\n",
    "# Selected column: Exterior2nd_AsbShng\n",
    "# Selected column: Exterior2nd_CBlock\n",
    "# Selected column: Exterior2nd_HdBoard\n",
    "# Selected column: Exterior2nd_MetalSd\n",
    "# Selected column: ExterCond_Ex\n",
    "# Selected column: BsmtQual_Ex\n",
    "# Selected column: BsmtExposure_Gd\n",
    "# Selected column: BsmtExposure_No\n",
    "# Selected column: BsmtFinType1_ALQ\n",
    "# Selected column: BsmtFinType1_GLQ\n",
    "# Selected column: BsmtFinType1_Unf\n",
    "# Selected column: Heating_GasW\n",
    "# Selected column: Electrical_FuseP\n",
    "# Selected column: Electrical_SBrkr\n",
    "# Selected column: KitchenQual_Ex\n",
    "# Selected column: Functional_Sev\n",
    "# Selected column: GarageFinish_RFn\n",
    "# Selected column: GarageFinish_Unf\n",
    "# Selected column: GarageCond_Po\n",
    "# Selected column: PavedDrive_Y\n",
    "# Selected column: Fence_GdPrv\n",
    "# Selected column: MiscFeature_Shed\n",
    "# Selected column: MiscFeature_TenC\n",
    "\n",
    "# Num Vars 15\n",
    "\n",
    "# Selected column: OverallQual\n",
    "# Selected column: MasVnrArea\n",
    "# Selected column: TotalBsmtSF\n",
    "# Selected column: GrLivArea\n",
    "# Selected column: GarageCars\n",
    "# Selected column: Neighborhood_NWAmes\n",
    "# Selected column: Neighborhood_NoRidge\n",
    "# Selected column: Neighborhood_NridgHt\n",
    "# Selected column: Exterior1st_CBlock\n",
    "# Selected column: ExterQual_TA\n",
    "# Selected column: Foundation_PConc\n",
    "# Selected column: BsmtFinType1_GLQ\n",
    "# Selected column: KitchenQual_Ex\n",
    "# Selected column: KitchenQual_TA\n",
    "# Selected column: GarageType_BuiltIn\n",
    "\n",
    "# Possible\n",
    "# Halfbalth, Fullbath\n",
    "\n",
    "# Definitely\n",
    "# Year remodeled - Use binary before 1990, and after 1990\n",
    "# Screen porch\n",
    "# Overall Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0c094c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572d40b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
